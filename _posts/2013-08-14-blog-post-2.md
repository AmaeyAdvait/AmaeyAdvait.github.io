---
title: 'Neuromorphic Computing'
date: 2025-11-25
permalink: /posts/2025/11/neuromorphic_computing/
tags:
  - cool posts
  - category1
  - category2
---

“Electronic circuits are millions of times faster than our biological circuits. At first we will have to devote all of this speed increase to compensating for the relative lack of parallelism in our computers, but ultimately the digital neocortex will be much faster than the biological variety and will only continue to increase in speed.”
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*~Ray Kurzweil, How to Create a Mind*

***This blog post is designed to give a pithy perspective into the fascinating world of neuromorphic Computing and what it specifically means to me.***

NeuroAI Systems(Part 1)
=======================

---
When we think about a system in electronics, we are always interested in finding its transfer function, power spectral density etc. But the crucial idea that tends to get missed is that at a fundamental level, the system exists without the signal and the signal exists without the system. In other terms, the signal and the system are indpendent of each other. Always. 

Yet, it seems hard to make sense of one without the other. For example, the neocortex is known to be active even without input, which is crucial for memory consolidation, but its predictions remain idle in a world where the information an event carries is inversely proportional to the probability of its occurence. On the flipside, many signals generated from various sources may exist in the vacuum of space, never passing through a brain and thus serving as remnants of uninterpreted possibilities. 

Thus, I believe that intelligence is deeply rooted in enactive cognition/predictive coding, which removes the focus from the system or the signal by themselves and places it on the interaction and embodiment between the two. 

Reference: [Emergence of simple-cell receptive field properties by learning a sparse code for natural images(Olshausen, Field)](https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf)

An interesting Article: [What is Mind?](https://www.mindandlife.org/insight/what-is-mind/)


As an engineer, this is exactly why neuromorphic computing is such a powerful tool. Focusing on building a system that characterizes the signal to noise ratio and other properties, researchers are trying to map the features of the brain onto a silicon wafer, to condense the working of tens of thousands of neurons into a very small area. 

Relating the signals and systems picture back to NeuroAI, neuromorphic computing is the pivotal step in visualising the shift of focus from AI(the signal) or the brain(the system) to their integrated response. With improved efficiency and temporal reasoning, neuromorphic computing is the key substrate that closes the virtuous feedback loop of NeuroAI. They are two faces of the same coin.

---

A Fourier Thought Experiment
=============================

We know that brains perform transforms all the time in order to predict patterns. But the methods employed by the brain to reach the frequency domain from the time domain are quite distinct when compared to a traditional discrete fourier transform/fast fourier transform. To bridge this gap, consider the following scenario:-

*<u>Building a brain inspired fast fourier transform (FFT)</u>: "An inquisitive student notices that the fast fourier transform's twiddle factor in the butterfly operations is inherently assumed to be static( w<sub>N</sub><sup>k</sup> {% raw %}$$ = e^{-i2pik/N}$${% endraw %}. Inspired by some machine learning courses that he took, he thus wonders if introducing an stochastic noisy learnable/trainable parameter ({% raw %}$$\theta$${% endraw %}<sub>k</sub>) to the classical twiddle factor and persforming a subsequent gradient descent will improve the robustness and adaptivity of the performance of the fft to noisy signals. Discuss the energy efficiency of the resulting FFT. Will the SNR of the ensuing brain inspired fft increase? What about the time complexity?"*

Notice that the parameter introduced can be made event driven i.e, for a sparse input signal with the zero norm << N, the system can afford to rest in specific periods to save energy, hence improving energy efficiency and reducing runtime. 

Another point to also consider is that (Plancherel's Theorem)'s validity for a brain inspired fft is probably questionable, since the basis vectors are no longer orthogonal at every instance. Hence, choosing the learning parameters is not only a question if minimizing a loss function, but also trying to ensure that the L<sub>2</sub> norm of the the final array of inner products of twiddle factors is as close to zero as possible. 

However, there are some cases where this could be a helpful feature, especially in converging to a stable signal and allowing the noise to fade away. Thu, the SNR of the resulting signal is significantly better compared to a classical fft.

The time complexity is interesting to think about, because adding a gradient descent algorithm certainly increases the time complexity while event driven computations for a sparse signal like a neural signal reduces its time complexity. I scoured the internet to find any research papers/blogs on adaptive twiddle factors, but couldn't find anything conducive, which suggests I might be treading relatively novel grounds here. 

I am planning to implement this brain inspired fft over this winter break, excited to see what comes out of it!

---

Finally, I cannot end the discussion on this neuroAI system without comments on some exciting advancements in it. Intel's Loihi 2 (also called Pohoiki Beach) developed by Mike Davies and his team manages to incorporate asynchronous event based spikes into the parallel sparse compute algorithm. They also launched Lava, an open source neuromorphic computing framework for developing neuro-inspired applications.

   Reference: [Loihi 2](https://www.intel.com/content/www/us/en/research/neuromorphic-computing-loihi-2-technology-brief.html)

   An excerpt from The Thinking Game documentary which particularly resonated with me: 
   (22:23-26:46)[The Thinking Game excerpt](https://youtu.be/d95J8yzvjbQ?si=TmJ5N5bGxOuPLVom&t=1343)













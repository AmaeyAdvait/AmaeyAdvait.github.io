---
title: 'Natural Language Processing'
date: 2025-11-26
permalink: /posts/2025/11/nlp/
tags:
  - cool posts
  - category1
  - category2
---

"...You had a point about language. When you get right down to it, it’s a work-around. Like trying to describe dreams with smoke signals. It’s noble, it’s maybe the most noble thing a body can do but you can’t turn a sunset into a string of grunts without losing something."
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*~Siri Keeton, Blindsight(Peter Watts)*

---

***This blog post is designed to give a pithy perspective into the fascinating world of Natural Language Processing and what it specifically means to me.***

NeuroAI Systems (Part 3)
========================

Every human can produce and understand different combinations of words that they haven't read or heard before. This almost certainly implies that our brains must have some programme that can build infinite sentences out of finite words. It's interesting to observe that we usually pay attention to what we are saying, but often neglect how we are saying it.

Ok, but why consider Natural Language Processing as a NeuroAI system? Because language is a wonderful embodiment of 'tacit' intelligence in the brain.

Let me explain.

Language is the bridge between thought and action. From the time I was a young child, I remember being fascinated by words, stories and the human brain's wonderful ability to piece together complex thoughts into meaningful strings. It is the purest element of any civilisation and without a doubt, the most essential as well. But while large language models and similar paradigms have managed to crack the enigma of language to convincingly show that language is not a uniquely human ability, we still have close to no understanding of how we can simulate thought. 

I believe this could be due to the inherently static nature of human language. While humans have gone about improving every aspect of their lives, language and its superficial grammar have remained more or less the same barring some additions to the vocabulary every couple of years. But it's important for us to understand that language can be improved. The universal grammar that each of us possess should not stop us from researching more efficient languages. I strongly feel that it's time to make language dynamic.

---

A Dynamic Thought Experiment
============================

How do we approach making language dynamic? We know that thoughts are continuous but language is made up of discrete units. Consider the following proposition:-

*"<u>Dynamic Language</u>: Phonemes are basic units of sounds that are sensed mentally, rather than spoken or heard. They represent cognitive categories. In order to explore a more flexible model of language, an analytical student decides to map each discrete phoneme to a continuous random variable and feed it to a machine. In simpler terms, she decides that since alphabets are static symbols, mapping clusters of them(morphemes/words) onto a stochastic process and defining their ensemble as a language improves the machine's dynamicity. Will this improve the machine's framework of language? Is her idea capable of capturing context and patterns which are vital to human language?"*

Let a language be composed of N discrete phonemes L = {w<sub>1</sub>, w<sub>2</sub>....,w<sub>N</sub>). Each w<sub>i</sub> is mapped to a high dimensional sparse continuous random process. Thus, we have a continuous semantic space where meanings can be represented through linear transformations for every iteration of mapping.

This approach gives us infinite variability, an infinite vocabulary and infinite expressivity. Further, from a cryptography perspective, continuous language models are definitely harder to crack, since many random variables cannot be inverted easily, especially if they are densely correlated. Similar ideas have already been implemented in supercomputer architectures. 

Reference: [Kanerva's Sparse Distributed Memory](https://ntrs.nasa.gov/api/citations/19890017031/downloads/19890017031.pdf)

Thinking of language as a signal, continuous language models would also be more robust to noise, since slightly distorted signals can still be reliably reconstructed using sampling and digital filter designs. We could also include gestures and facial expressions since they mesh well with continuous adaptivity. 

Finally, it is compatible with brain-like computation. However, incorporating this into human language is probably impractical atleast on a forseeable timescale , since humans are hardwired to think about language in logical, discrete units. If we are able to find ways to successfully integrate mind and machine, continuous language models could revolutionise the way we think about language, commnication and memories.

Again, it feels like I'm treading relatively new territory here, but implementing this idea through well defined algorithms likely requires some deeper understanding of concepts in machine learning, formal language and a dozen sparks of ingenuity which still lie some distance ahead in my journey. Looking forward to continous progress beyond this preliminary vision!

With this, we come to an end of our journey exploring neuroAI systems. In the next blog, we will discuss some insights into building human centered AI systems and how both systems are deeply interconnected.
